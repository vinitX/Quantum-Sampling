{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7ff29ae-43cb-4557-8c72-f92582169082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"c:\\\\Users\\\\vinit\\\\Downloads\\\\Research\\\\Quantum-Sampling\\\\Manas' Proposals\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from pyscf import gto, scf, fci, lo\n",
    "#import netket as nk; import netket.experimental as nkx\n",
    "import numpy as np\n",
    "import time\n",
    "#import itertools\n",
    "import qiskit\n",
    "#from qiskit.quantum_info import Pauli, SparsePauliOp\n",
    "from collections import defaultdict\n",
    "#import tensorflow as tf\n",
    "#import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "#import itertools\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "#sys.path.append(os.path.dirname(os.getcwd()) + \"/SAMPLER_LOCAL_IMPORT\")\n",
    "#from Sampling_Quantum import *\n",
    "#from New_MCMC_Proposal import *\n",
    "#sys.path.append(os.getcwd() + \"/Code_download_Bell_2\")\n",
    "from MCMC_funs_Leyden import *\n",
    "\n",
    "print(qiskit.version.get_version_info())\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c16698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b61c6d12-f2be-40af-92a7-ef64965dabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_configs(n):\n",
    "    return [np.array(v) for v in product([-1, 1], repeat=n)]\n",
    "\n",
    "\n",
    "\n",
    "def Sampling_MCMC_trajectories(problem_inst, Transition_matrix, sample_size=10000, \n",
    "                               burn=1000, method='Quantum', init_config=None):\n",
    "\n",
    "    n = problem_inst.n\n",
    "    beta = problem_inst.T\n",
    "    prob_dist = np.zeros(2**n)\n",
    "\n",
    "    #exact_dist = np.exp(-beta * Proposal_object.Energy_array)\n",
    "    #exact_dist = exact_dist / np.sum(exact_dist)\n",
    "\n",
    "    #err_hist = []\n",
    "    key_list = []\n",
    "\n",
    "    if init_config==None:\n",
    "        s = np.random.choice([1,-1],size=n)\n",
    "    else: s = init_config\n",
    "\n",
    "    int_key = spinconf2int(s)\n",
    "    #print(int_key)\n",
    "    \n",
    "    for k in range(burn):\n",
    "        #s = Proposal_object.generate_MCMC_trajectories(Transition_matrix, s)\n",
    "         int_key = generate_move(transition_mat=Transition_matrix, state=int_key)\n",
    "\n",
    "    for k in range(sample_size):\n",
    "        #s = Proposal_object.generate_MCMC_trajectories(Transition_matrix, s)\n",
    "        int_key = generate_move(transition_mat=Transition_matrix, state=int_key)\n",
    "        #key = spinconf2int(s)\n",
    "        prob_dist[int_key] +=1\n",
    "        key_list.append(int_key)\n",
    "\n",
    "    \n",
    "    return np.flip(prob_dist/np.sum(prob_dist)), key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_U_mala(x, J, h, alpha):\n",
    "    v = np.tanh(alpha * x)\n",
    "    sech_sq = 1.0 * (1 - v ** 2)\n",
    "    return alpha * sech_sq * (h + 2 * J @ v)\n",
    "\n",
    "def normalize_transition_matrix(T, eps=1e-12, verbose=False):\n",
    "    T = T.copy()\n",
    "    T[T < 0] = 0.0\n",
    "    col_sums = T.sum(axis=0)\n",
    "    bad = np.abs(col_sums) <= eps\n",
    "    good = ~bad\n",
    "    if np.any(good):\n",
    "        T[:, good] /= col_sums[None, good]\n",
    "    if np.any(bad):\n",
    "        T[:, bad] = 1.0 / T.shape[0]\n",
    "        if verbose:\n",
    "            print(f\"{bad.sum()} columns had near-zero sum. Reset to uniform.\")\n",
    "    return T\n",
    "\n",
    "\n",
    "def Cov_computer(sample_vector_stacks):\n",
    "    #sample_vector_stacks is a row-wise stack of last k sample vectors. Each sample vector of length n\n",
    "    X = np.asarray(sample_vector_stacks, dtype=float)\n",
    "    #print(X)\n",
    "    if X.shape[0] <= 1:\n",
    "        return np.eye(X.shape[1])\n",
    "    mean = X.mean(axis=0)\n",
    "    #print(np.tile(mean, (X.shape[0],1)))\n",
    "    diffs = X - np.tile(mean, (X.shape[0],1))\n",
    "    return (diffs.T @ diffs) / (X.shape[0] - 1)\n",
    "\n",
    "\n",
    "def normal_dist_logpdf(y, mean, P, epsilon):\n",
    "    diff = y - mean\n",
    "    z = np.linalg.solve(P, diff)          # P z = diff  z = P^-1 diff\n",
    "    quad_form = float(z @ z)\n",
    "    logdet = 2.0*np.sum(np.log(np.diag(P))) + 2.0*y.size*np.log(epsilon)\n",
    "    return -0.5 * (y.size * np.log(2.0 * np.pi) + logdet + (epsilon**(-2))*quad_form)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f0e615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9747c21",
   "metadata": {},
   "source": [
    "## Non-Vectorized Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7f4b3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_MALA_step_new(x, P, cov, epsilon, problem_inst, alpha=3.0):\n",
    "    J_Q = problem_inst.J_quantum\n",
    "    h_Q = problem_inst.h_quantum\n",
    "    cov = cov + 1e-5 * np.eye(len(x))\n",
    "\n",
    "    mean_shift_at_xnew = x + (0.5*epsilon**2) * problem_inst.T * cov @ grad_U_mala(x, J_Q, h_Q, alpha)\n",
    "\n",
    "    #white_noise_term = np.random.multivariate_normal(np.zeros(len(x)), epsilon**2 * np.dot(P, np.conjugate(P).T))\n",
    "    x_new = np.random.multivariate_normal(mean=mean_shift_at_xnew, cov=epsilon**2 * np.dot(P, np.conjugate(P).T), size=1)[0]\n",
    "    #print(x_new)\n",
    "\n",
    "    mean_shift_at_xold = x_new + (0.5*epsilon**2) * problem_inst.T * cov @ grad_U_mala(x_new, J_Q, h_Q, alpha)\n",
    "\n",
    "    v_old = np.sign(np.tanh(alpha * x))\n",
    "    v_new = np.sign(np.tanh(alpha * x_new))\n",
    "    E_old = problem_inst.E_arr[::-1][spinconf2int(v_old)]\n",
    "    E_new = problem_inst.E_arr[::-1][spinconf2int(v_new)]\n",
    "\n",
    "    log_prop_dist_fwd = normal_dist_logpdf(x_new, mean_shift_at_xnew, P, epsilon)\n",
    "    log_prop_dist_rev = normal_dist_logpdf(x, mean_shift_at_xold, P, epsilon)\n",
    "\n",
    "    log_accept_ratio = -problem_inst.T * (E_new - E_old) + (log_prop_dist_rev - log_prop_dist_fwd)\n",
    "    accept_prob = min(1.0, np.exp(log_accept_ratio))\n",
    "\n",
    "    if np.random.rand() < accept_prob:\n",
    "        return x_new, v_new\n",
    "    else:\n",
    "        return x, v_old\n",
    "\n",
    "\n",
    "\n",
    "def adaptive_MALA_T_matrix(problem_inst, alpha=3.0, epsilon=0.3, num_samples=500):\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    n = problem_inst.n\n",
    "    #beta = problem_inst.T\n",
    "\n",
    "    configs = all_configs(n)\n",
    "    idx_map = {tuple(c): i for i, c in enumerate(configs)}\n",
    "    T = np.zeros((1 << n, 1 << n))\n",
    "\n",
    "    burn_in = 1000\n",
    "    #step_time_hist = []\n",
    "\n",
    "    for v in tqdm(configs, desc=\"Building T matrix\"):\n",
    "        x = np.arctanh(np.clip(v, -0.999, 0.999)) / alpha\n",
    "        cov = np.eye(n)\n",
    "        P = np.eye(n)\n",
    "\n",
    "        counts = defaultdict(int)\n",
    "\n",
    "        v_samples = []\n",
    "        x_samples = []\n",
    "        adapt_window = 20\n",
    "\n",
    "        tmg = time.time()\n",
    "\n",
    "        for step in range(1, num_samples+1):\n",
    "            #x, v_new = adaptive_MALA_step(x, cov, epsilon, problem_inst, alpha=3.0)\n",
    "            #tm = time.time()\n",
    "            x, v_new = adaptive_MALA_step_new(x, P, cov, epsilon, problem_inst, alpha=3.0)\n",
    "            #step_time_hist.append(time.time()-tm)\n",
    "\n",
    "            v_samples.append(v_new)\n",
    "            x_samples.append(x)\n",
    "\n",
    "            key = tuple(v_new.astype(int))\n",
    "            counts[key] = counts.get(key, 0) + 1\n",
    "\n",
    "            if (step) % adapt_window == 0 and step > 200 and step <= burn_in:\n",
    "                if len(x_samples) >= 80:\n",
    "                    #x_hist = np.array([np.arctanh(np.clip(np.tanh(alpha * x0), -0.999, 0.999)) / alpha for x0 in v_samples[-50:]])\n",
    "                    x_hist = [x for x in x_samples[-80:]]\n",
    "\n",
    "                else:\n",
    "                    #x_hist = np.array([np.arctanh(np.clip(np.tanh(alpha * x0), -0.999, 0.999)) / alpha for x0 in v_samples])\n",
    "                    x_hist = [x for x in x_samples[:]]\n",
    "\n",
    "                #cov_new = np.cov(np.array(x_hist).T) \n",
    "                #print(x_hist)\n",
    "\n",
    "                cov_new = Cov_computer(x_hist)\n",
    "                gamma = 0.1             # diminishing\n",
    "                cov = (1.0 - gamma) * cov_new + gamma * cov\n",
    "                cov = 0.5 * (cov + cov.T)\n",
    "                P = np.linalg.cholesky(cov) \n",
    "\n",
    "        # print(\"Average step time:\", np.mean(step_time_hist))\n",
    "        # print(\"Total time for config:\", time.time()-tmg)\n",
    "\n",
    "        # step_time_hist = np.array(step_time_hist)\n",
    "        \n",
    "        # step_time_hist[step_time_hist < 1e-10] = np.nan\n",
    "        \n",
    "        # plt.hist(step_time_hist, bins=30)\n",
    "        # plt.show()\n",
    "\n",
    "        i = idx_map[tuple(v)]\n",
    "        total = sum(counts.values())\n",
    "\n",
    "        for v_prime, c in counts.items():\n",
    "            j = idx_map[v_prime]\n",
    "\n",
    "            if j != i :\n",
    "                T[j, i] = c / total\n",
    "\n",
    "        T[i,i] = 1-sum(T[:,i])\n",
    "\n",
    "    T = normalize_transition_matrix(T, eps=1e-12, verbose=True)\n",
    "\n",
    "    return T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f32877",
   "metadata": {},
   "source": [
    "## Vectorized Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e121bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_U_mala_batch(X, J, H, alpha):\n",
    "    V = np.tanh(alpha * X)                      # (B, n)\n",
    "    sech_sq = 1.0 - V**2                        # (B, n)\n",
    "    return alpha * sech_sq * (H[None, :] + 2.0 * V @ J.T)\n",
    "\n",
    "def normal_dist_logpdf_batch(Y, Mean, P, epsilon):\n",
    "    B, n = Y.shape\n",
    "    diff = (Y - Mean)                            # (B, n)\n",
    "\n",
    "    Z = np.linalg.solve(P, diff.T)               # (n, B)\n",
    "    quad = np.sum(Z * Z, axis=0)                 # (B,)\n",
    "\n",
    "    logdet = 2.0 * np.sum(np.log(np.diag(P))) + 2.0 * n * np.log(epsilon)  # scalar\n",
    "    return -0.5 * (n * np.log(2.0 * np.pi) + logdet + (epsilon**(-2)) * quad)\n",
    "\n",
    "def spins_to_index(V):\n",
    "    \"\"\"\n",
    "    V: (B, n) spins in {-1, +1}\n",
    "    Returns (B,) indices \n",
    "    \"\"\"\n",
    "    V = np.asarray(V)\n",
    "    bits = (V < 0).astype(np.int64)                 # +1 -> 0, -1 -> 1\n",
    "    n = V.shape[1]\n",
    "    weights = (1 << np.arange(n-1, -1, -1, dtype=np.int64))  # MSB at column 0\n",
    "    return bits @ weights\n",
    "\n",
    "\n",
    "# ---------- Batched MALA step ----------\n",
    "\n",
    "def adaptive_MALA_step_new_batch(X, P, cov, epsilon, problem_inst, alpha=3.0):\n",
    "    B, n = X.shape\n",
    "    Tinv = problem_inst.T \n",
    "    J_Q = problem_inst.J_quantum\n",
    "    h_Q = problem_inst.h_quantum\n",
    "\n",
    "    grad_X = grad_U_mala_batch(X, J_Q, h_Q, alpha)                 # (B, n)\n",
    "    mean_fwd = X + 0.5 * (epsilon**2) * Tinv * (grad_X @ cov.T)  # (B, n)\n",
    "\n",
    "    # ----- Sample proposal with shared covariance epsilon^2 * P P^T -----\n",
    "    # Draw Z ~ N(0, I), then X_new = mean_fwd + epsilon * Z @ P^T\n",
    "    Z = np.random.randn(B, n)\n",
    "    X_new = mean_fwd + epsilon * (Z @ P.T)                          # (B, n)\n",
    "\n",
    "    grad_Xnew = grad_U_mala_batch(X_new, J_Q, h_Q, alpha)           # (B, n)\n",
    "    mean_rev = X_new + 0.5 * (epsilon**2) * Tinv * (grad_Xnew @ cov.T)\n",
    "\n",
    "    V_old = np.sign(np.tanh(alpha * X))                             # (B, n), in {-1, +1}\n",
    "    V_new = np.sign(np.tanh(alpha * X_new))\n",
    "\n",
    "    idx_old = spins_to_index(V_old)\n",
    "    idx_new = spins_to_index(V_new)\n",
    "    E_table = problem_inst.E_arr[::-1]                    \n",
    "    E_old = E_table[idx_old]                                        \n",
    "    E_new = E_table[idx_new]                                        \n",
    "\n",
    "    log_q_fwd = normal_dist_logpdf_batch(X_new, mean_fwd, P, epsilon)    \n",
    "    log_q_rev = normal_dist_logpdf_batch(X,     mean_rev, P, epsilon)    \n",
    "\n",
    "    log_acc = -Tinv * (E_new - E_old) + (log_q_rev - log_q_fwd)     \n",
    "    acc_prob = np.clip(np.exp(np.minimum(0.0, log_acc)), 0.0, 1.0)  \n",
    "    accept_mask = (np.random.rand(B) < acc_prob)\n",
    "\n",
    "    # Apply accepts with boolean mask\n",
    "    X_next = np.where(accept_mask[:, None], X_new, X)\n",
    "    V_next = np.where(accept_mask[:, None], V_new, V_old)\n",
    "\n",
    "    return X_next, V_next\n",
    "\n",
    "\n",
    "\n",
    "def adaptive_MALA_T_matrix_vec(problem_inst, alpha=3.0, epsilon=0.3, num_samples=500):\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    n = problem_inst.n\n",
    "\n",
    "    V = all_configs(n)\n",
    "    T = np.zeros((1 << n, 1 << n))\n",
    "\n",
    "    burn_in = 1000\n",
    "\n",
    "    X = np.arctanh(np.clip(V, -0.999, 0.999)) / alpha\n",
    "    cov = np.eye(n)\n",
    "    P = np.eye(n)\n",
    "\n",
    "    counts = defaultdict(int)\n",
    "    counts_mat = np.zeros((1<<n, 1<<n), dtype=np.int64)\n",
    "\n",
    "    adapt_window = 20\n",
    "    cov_samples = 20                                  # keep last K samples for covariance computation\n",
    "\n",
    "    # --- rolling buffer of shape (K, B, n) ---\n",
    "    buffer = np.empty((cov_samples, 2**n, n), dtype=float)\n",
    "    buf_idx = 0          # next write position\n",
    "\n",
    "    tmg = time.time()\n",
    "\n",
    "    for step in tqdm(range(1, num_samples+1)):\n",
    "        origin_idx = spins_to_index(V)   \n",
    "        X, V = adaptive_MALA_step_new_batch(X, P, cov, epsilon, problem_inst, alpha=3.0)\n",
    "        dest_idx = spins_to_index(V)    \n",
    "        #x, v_new = adaptive_MALA_step_new(x, P, cov, epsilon, problem_inst, alpha=3.0)\n",
    "\n",
    "        np.add.at(counts_mat, (dest_idx, origin_idx), 1)\n",
    "\n",
    "        buffer[buf_idx] = X\n",
    "        buf_idx = (buf_idx + 1) % cov_samples\n",
    "\n",
    "        # update counts (vectorized)\n",
    "        uniq, cnts = np.unique(V.astype(int), axis=0, return_counts=True)\n",
    "        for row, c in zip(uniq, cnts):\n",
    "            counts[tuple(row.tolist())] = counts.get(tuple(row.tolist()), 0) + int(c)\n",
    "\n",
    "        # adapt covariance on schedule\n",
    "        if (step % adapt_window == 0) and (step > 100) and (step <= burn_in):\n",
    "            X_hist = buffer.reshape(-1, n)                           # ((cov_samples*B), n)\n",
    "            cov_new = Cov_computer(X_hist)\n",
    "            gamma = 0.1\n",
    "            cov = (1.0 - gamma) * cov_new + gamma * cov\n",
    "            cov = 0.5 * (cov + cov.T)\n",
    "            cov += 1e-6 * np.eye(n)                                  # for stability\n",
    "            P = np.linalg.cholesky(cov)\n",
    "\n",
    "    print(\"Total time for config:\", time.time()-tmg)\n",
    "\n",
    "    T = normalize_transition_matrix(np.array(counts_mat, dtype=float), eps=1e-12, verbose=True)\n",
    "\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7cc36a91-7a55-49db-9870-dd9c8f9e7d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "Starting problem instance 8 of 1 with n = 10\n",
      "Warning: J_Q is not symmetric, correcting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 131.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for config: 7.616300821304321\n",
      "Instance 8, T = 1.0, Proposal = MALA_MC, Acceptance = metropolis, Gap = 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinit\\AppData\\Local\\Temp\\ipykernel_43360\\4139955128.py:67: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results,pd.DataFrame([results_datum])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "instances = pd.read_pickle('instances_new.pkl')\n",
    "tot_instances = np.max(instances['Instance Number'].values)\n",
    "print(tot_instances)\n",
    "tot_instances = 1\n",
    "results = pd.DataFrame(columns = [\n",
    "    'Instance Number',\n",
    "    'Spins', \n",
    "    'Temperature', \n",
    "    'Connectivity',\n",
    "    'Proposal',\n",
    "    'Acceptance',\n",
    "    'Gap',\n",
    "    'Gap Lazy'\n",
    "        ])\n",
    "T_logmesh = np.logspace(-3, 3, 50)\n",
    "T_round = np.sort( np.kron(np.array([1,5]), 10.**np.arange(-3,4)))\n",
    "T_lim = np.array([0, np.inf]) \n",
    "T_arr = np.unique( np.concatenate((T_logmesh, T_round, T_lim))) \n",
    "#T_arr = np.delete(T_arr, -2) \n",
    "T_arr = [1]\n",
    "delta_step = 0.2\n",
    "\n",
    "for instance_num in range(1, tot_instances+1):\n",
    "    instance_num = 8\n",
    "\n",
    "    cond  = (instances['Instance Number']==instance_num)\n",
    "    n            = instances[cond]['Spins'].values[0]\n",
    "    connectivity = instances[cond]['Connectivity'].values[0]\n",
    "    J            = instances[cond]['J'].values[0]\n",
    "    h            = instances[cond]['h'].values[0]\n",
    "\n",
    "    print('Starting problem instance', instance_num, 'of', tot_instances, 'with n =', n)\n",
    "\n",
    "    problem_inst = ProblemInstance(J, h)\n",
    "    J_Q = problem_inst.J_quantum\n",
    "    if np.linalg.norm(J_Q - J_Q.T) > 1e-8:\n",
    "        print(\"Warning: J_Q is not symmetric, correcting...\")\n",
    "        problem_inst.J_quantum = (problem_inst.J_quantum + problem_inst.J_quantum.T)  # Ensure symmetry\n",
    "\n",
    "    problem_inst.T = 1.0\n",
    "\n",
    "    proposal_mats = {}\n",
    "\n",
    "    T = 1.0\n",
    "    prop_type = \"MALA_MC\"\n",
    "    accept_type = 'metropolis'\n",
    "\n",
    "\n",
    "    #transition_mat = adaptive_MALA_T_matrix(problem_inst, alpha=3.0, epsilon=0.2, num_samples=1000)\n",
    "    transition_mat = adaptive_MALA_T_matrix_vec(problem_inst, alpha=3.0, epsilon=0.2, num_samples=1000)\n",
    "\n",
    "    gap, _ = abs_spectral_gap(transition_mat)\n",
    "\n",
    "    results_datum = {\n",
    "        'Instance Number': instance_num,\n",
    "        'Spins': n, \n",
    "        'Temperature': T, \n",
    "        'Connectivity': connectivity,\n",
    "        'Proposal': prop_type,\n",
    "        'Acceptance': accept_type,\n",
    "        'Gap': gap,\n",
    "        \"delta_time_step_Trotter\": delta_step\n",
    "    }\n",
    "\n",
    "\n",
    "    results = pd.concat([results,pd.DataFrame([results_datum])], ignore_index=True)\n",
    "\n",
    "    print(\"Instance {}, T = {}, Proposal = {}, Acceptance = {}, Gap = {:.4f}\".format(\n",
    "        instance_num, T, prop_type, accept_type, gap))\n",
    "\n",
    "    #results = pd.concat([results,pd.DataFrame([results_datum])], ignore_index=True)\n",
    "\n",
    "    results_row = pd.DataFrame([results_datum])\n",
    "    results_row.to_csv('results_MALA.csv', mode='a', header=not os.path.exists('results_MALA.csv'), index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9825d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
